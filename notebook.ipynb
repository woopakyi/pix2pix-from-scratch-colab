{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##a) models/networks.py → Define UNetGenerator and NLayerDiscriminator classes"
      ],
      "metadata": {
        "id": "0pQWOw6SHK-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8FIFyRLpI1v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Helper Functions\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_norm_layer(norm_type=\"instance\"):\n",
        "    \"\"\"Return a normalization layer\n",
        "\n",
        "    Parameters:\n",
        "        norm_type (str) -- the name of the normalization layer: batch | instance | none\n",
        "\n",
        "    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n",
        "    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n",
        "    \"\"\"\n",
        "    if norm_type == \"batch\":\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
        "    elif norm_type == \"syncbatch\":\n",
        "        norm_layer = functools.partial(nn.SyncBatchNorm, affine=True, track_running_stats=True)\n",
        "    elif norm_type == \"instance\":\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
        "    elif norm_type == \"none\":\n",
        "\n",
        "        def norm_layer(x):\n",
        "            return Identity()\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\"normalization layer [%s] is not found\" % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, opt):\n",
        "    \"\"\"Return a learning rate scheduler\n",
        "\n",
        "    Parameters:\n",
        "        optimizer          -- the optimizer of the network\n",
        "        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．\n",
        "                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n",
        "\n",
        "    For 'linear', we keep the same learning rate for the first <opt.n_epochs> epochs\n",
        "    and linearly decay the rate to zero over the next <opt.n_epochs_decay> epochs.\n",
        "    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n",
        "    See https://pytorch.org/docs/stable/optim.html for more details.\n",
        "    \"\"\"\n",
        "    if opt.lr_policy == \"linear\":\n",
        "\n",
        "        def lambda_rule(epoch):\n",
        "            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.n_epochs) / float(opt.n_epochs_decay + 1)\n",
        "            return lr_l\n",
        "\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    elif opt.lr_policy == \"step\":\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
        "    elif opt.lr_policy == \"plateau\":\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, threshold=0.01, patience=5)\n",
        "    elif opt.lr_policy == \"cosine\":\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.n_epochs, eta_min=0)\n",
        "    else:\n",
        "        return NotImplementedError(\"learning rate policy [%s] is not implemented\", opt.lr_policy)\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def init_weights(net, init_type=\"normal\", init_gain=0.02):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Parameters:\n",
        "        net (network)   -- network to be initialized\n",
        "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
        "    work better for some applications. Feel free to try yourself.\n",
        "    \"\"\"\n",
        "\n",
        "    def init_func(m):  # define the initialization function\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, \"weight\") and (classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1):\n",
        "            if init_type == \"normal\":\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == \"xavier\":\n",
        "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
        "            elif init_type == \"kaiming\":\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_in\")\n",
        "            elif init_type == \"orthogonal\":\n",
        "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError(\"initialization method [%s] is not implemented\" % init_type)\n",
        "            if hasattr(m, \"bias\") and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find(\"BatchNorm2d\") != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
        "            init.normal_(m.weight.data, 1.0, init_gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print(\"initialize network with %s\" % init_type)\n",
        "    net.apply(init_func)  # apply the initialization function <init_func>\n",
        "\n",
        "\n",
        "def init_net(net, init_type=\"normal\", init_gain=0.02):\n",
        "    \"\"\"Initialize a network: 1. register CPU/GPU device; 2. initialize the network weights\n",
        "    Parameters:\n",
        "        net (network)      -- the network to be initialized\n",
        "        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    Return an initialized network.\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if \"LOCAL_RANK\" in os.environ:\n",
        "            local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "            net.to(local_rank)\n",
        "            print(f\"Initialized with device cuda:{local_rank}\")\n",
        "        else:\n",
        "            net.to(0)\n",
        "            print(\"Initialized with device cuda:0\")\n",
        "    init_weights(net, init_type, init_gain=init_gain)\n",
        "    return net\n",
        "\n",
        "\n",
        "def define_G(input_nc, output_nc, ngf, netG, norm=\"batch\", use_dropout=False, init_type=\"normal\", init_gain=0.02):\n",
        "    \"\"\"Create a generator\n",
        "\n",
        "    Parameters:\n",
        "        input_nc (int) -- the number of channels in input images\n",
        "        output_nc (int) -- the number of channels in output images\n",
        "        ngf (int) -- the number of filters in the last conv layer\n",
        "        netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_128 | unet_256\n",
        "        norm (str) -- the name of normalization layers used in the network: batch | instance | none\n",
        "        use_dropout (bool) -- if use dropout layers.\n",
        "        init_type (str)    -- the name of our initialization method.\n",
        "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    Returns a generator\n",
        "    \"\"\"\n",
        "    net = None\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\n",
        "\n",
        "    if netG == \"resnet_9blocks\":\n",
        "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n",
        "    elif netG == \"resnet_6blocks\":\n",
        "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n",
        "    elif netG == \"unet_128\":\n",
        "        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "    elif netG == \"unet_256\":\n",
        "        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Generator model name [%s] is not recognized\" % netG)\n",
        "    return net\n",
        "\n",
        "\n",
        "def define_D(input_nc, ndf, netD, n_layers_D=3, norm=\"batch\", init_type=\"normal\", init_gain=0.02):\n",
        "    \"\"\"Create a discriminator\n",
        "\n",
        "    Parameters:\n",
        "        input_nc (int)     -- the number of channels in input images\n",
        "        ndf (int)          -- the number of filters in the first conv layer\n",
        "        netD (str)         -- the architecture's name: basic | n_layers | pixel\n",
        "        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'\n",
        "        norm (str)         -- the type of normalization layers used in the network.\n",
        "        init_type (str)    -- the name of the initialization method.\n",
        "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    Returns a discriminator\n",
        "\n",
        "    Our current implementation provides three types of discriminators:\n",
        "        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.\n",
        "        It can classify whether 70×70 overlapping patches are real or fake.\n",
        "        Such a patch-level discriminator architecture has fewer parameters\n",
        "        than a full-image discriminator and can work on arbitrarily-sized images\n",
        "        in a fully convolutional fashion.\n",
        "\n",
        "        [n_layers]: With this mode, you can specify the number of conv layers in the discriminator\n",
        "        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n",
        "\n",
        "        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n",
        "        It encourages greater color diversity but has no effect on spatial statistics.\n",
        "\n",
        "    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n",
        "    \"\"\"\n",
        "    net = None\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\n",
        "\n",
        "    if netD == \"basic\":  # default PatchGAN classifier\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)\n",
        "    elif netD == \"n_layers\":  # more options\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n",
        "    elif netD == \"pixel\":  # classify if each pixel is real or fake\n",
        "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Discriminator model name [%s] is not recognized\" % netD)\n",
        "    return net\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# Classes\n",
        "##############################################################################\n",
        "class GANLoss(nn.Module):\n",
        "    \"\"\"Define different GAN objectives.\n",
        "\n",
        "    The GANLoss class abstracts away the need to create the target label tensor\n",
        "    that has the same size as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
        "        \"\"\"Initialize the GANLoss class.\n",
        "\n",
        "        Parameters:\n",
        "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
        "            target_real_label (bool) - - label for a real image\n",
        "            target_fake_label (bool) - - label of a fake image\n",
        "\n",
        "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
        "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
        "        \"\"\"\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer(\"real_label\", torch.tensor(target_real_label))\n",
        "        self.register_buffer(\"fake_label\", torch.tensor(target_fake_label))\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == \"lsgan\":\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == \"vanilla\":\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode in [\"wgangp\"]:\n",
        "            self.loss = None\n",
        "        else:\n",
        "            raise NotImplementedError(\"gan mode %s not implemented\" % gan_mode)\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Create label tensors with the same size as the input.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            A label tensor filled with ground truth label, and with the size of the input\n",
        "        \"\"\"\n",
        "\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        if self.gan_mode in [\"lsgan\", \"vanilla\"]:\n",
        "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "            loss = self.loss(prediction, target_tensor)\n",
        "        elif self.gan_mode == \"wgangp\":\n",
        "            if target_is_real:\n",
        "                loss = -prediction.mean()\n",
        "            else:\n",
        "                loss = prediction.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def cal_gradient_penalty(netD, real_data, fake_data, device, type=\"mixed\", constant=1.0, lambda_gp=10.0):\n",
        "    \"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n",
        "\n",
        "    Arguments:\n",
        "        netD (network)              -- discriminator network\n",
        "        real_data (tensor array)    -- real images\n",
        "        fake_data (tensor array)    -- generated images from the generator\n",
        "        device (str)                -- GPU / CPU\n",
        "        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n",
        "        constant (float)            -- the constant used in formula ( ||gradient||_2 - constant)^2\n",
        "        lambda_gp (float)           -- weight for this loss\n",
        "\n",
        "    Returns the gradient penalty loss\n",
        "    \"\"\"\n",
        "    if lambda_gp > 0.0:\n",
        "        if type == \"real\":  # either use real images, fake images, or a linear interpolation of two.\n",
        "            interpolatesv = real_data\n",
        "        elif type == \"fake\":\n",
        "            interpolatesv = fake_data\n",
        "        elif type == \"mixed\":\n",
        "            alpha = torch.rand(real_data.shape[0], 1, device=device)\n",
        "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n",
        "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"{type} not implemented\")\n",
        "        interpolatesv.requires_grad_(True)\n",
        "        disc_interpolates = netD(interpolatesv)\n",
        "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv, grad_outputs=torch.ones(disc_interpolates.size()).to(device), create_graph=True, retain_graph=True, only_inputs=True)\n",
        "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
        "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp  # added eps\n",
        "        return gradient_penalty, gradients\n",
        "    else:\n",
        "        return 0.0, None\n",
        "\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
        "\n",
        "    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type=\"reflect\"):\n",
        "        \"\"\"Construct a Resnet-based generator\n",
        "\n",
        "        Parameters:\n",
        "            input_nc (int)      -- the number of channels in input images\n",
        "            output_nc (int)     -- the number of channels in output images\n",
        "            ngf (int)           -- the number of filters in the last conv layer\n",
        "            norm_layer          -- normalization layer\n",
        "            use_dropout (bool)  -- if use dropout layers\n",
        "            n_blocks (int)      -- the number of ResNet blocks\n",
        "            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n",
        "        \"\"\"\n",
        "        assert n_blocks >= 0\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias), norm_layer(ngf), nn.ReLU(True)]\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):  # add downsampling layers\n",
        "            mult = 2**i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias), norm_layer(ngf * mult * 2), nn.ReLU(True)]\n",
        "\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):  # add ResNet blocks\n",
        "\n",
        "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
        "\n",
        "        for i in range(n_downsampling):  # add upsampling layers\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias), norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward\"\"\"\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"Define a Resnet block\"\"\"\n",
        "\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        \"\"\"Initialize the Resnet block\n",
        "\n",
        "        A resnet block is a conv block with skip connections\n",
        "        We construct a conv block with build_conv_block function,\n",
        "        and implement skip connections in <forward> function.\n",
        "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
        "        \"\"\"\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        \"\"\"Construct a convolutional block.\n",
        "\n",
        "        Parameters:\n",
        "            dim (int)           -- the number of channels in the conv layer.\n",
        "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
        "            norm_layer          -- normalization layer\n",
        "            use_dropout (bool)  -- if use dropout layers.\n",
        "            use_bias (bool)     -- if the conv layer uses bias or not\n",
        "\n",
        "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
        "        \"\"\"\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == \"reflect\":\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == \"replicate\":\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == \"zero\":\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError(\"padding [%s] is not implemented\" % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == \"reflect\":\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == \"replicate\":\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == \"zero\":\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError(\"padding [%s] is not implemented\" % padding_type)\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function (with skip connections)\"\"\"\n",
        "        out = x + self.conv_block(x)  # add skip connections\n",
        "        return out\n",
        "\n",
        "\n",
        "class UnetGenerator(nn.Module):\n",
        "    \"\"\"Create a Unet-based generator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        \"\"\"Construct a Unet generator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            output_nc (int) -- the number of channels in output images\n",
        "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
        "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
        "            ngf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "\n",
        "        We construct the U-Net from the innermost layer to the outermost layer.\n",
        "        It is a recursive process.\n",
        "        \"\"\"\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
        "        for i in range(num_downs - 5):  # add intermediate layers with ngf * 8 filters\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward\"\"\"\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    \"\"\"Defines the Unet submodule with skip connection.\n",
        "    X -------------------identity----------------------\n",
        "    |-- downsampling -- |submodule| -- upsampling --|\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None, submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        \"\"\"Construct a Unet submodule with skip connections.\n",
        "\n",
        "        Parameters:\n",
        "            outer_nc (int) -- the number of filters in the outer conv layer\n",
        "            inner_nc (int) -- the number of filters in the inner conv layer\n",
        "            input_nc (int) -- the number of channels in input images/features\n",
        "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
        "            outermost (bool)    -- if this module is the outermost module\n",
        "            innermost (bool)    -- if this module is the innermost module\n",
        "            norm_layer          -- normalization layer\n",
        "            use_dropout (bool)  -- if use dropout layers.\n",
        "        \"\"\"\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:  # add skip connections\n",
        "            return torch.cat([x, self.model(x)], 1)\n",
        "\n",
        "\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2**n, 8)\n",
        "            sequence += [nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2**n_layers, 8)\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class PixelDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
        "\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        self.net = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_layer(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "        ]\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.net(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(self):\n",
        "    \"\"\"Return intermediate feature maps for debugging\"\"\"\n",
        "    return {\n",
        "        'enc1': self.model[1].output if hasattr(self, 'model') else None,  # adjust indices as needed\n",
        "        'enc4': self.down4.output,\n",
        "        'bottleneck': self.bottleneck.output,\n",
        "        'dec4': self.up1[0].output,\n",
        "    }"
      ],
      "metadata": {
        "id": "cDpKwAZImEmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##b) models/pix2pix_model.py → Define the Pix2PixModel (or separately define Generator and Discriminator)"
      ],
      "metadata": {
        "id": "JEHpY3xwHRfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
        "%cd pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/pytorch-CycleGAN-and-pix2pix')"
      ],
      "metadata": {
        "id": "UA84es_LJUdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.base_model import BaseModel\n",
        "import models.networks as networks\n",
        "\n",
        "\n",
        "class Pix2PixModel(BaseModel):\n",
        "    \"\"\"This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n",
        "\n",
        "    The model training requires '--dataset_mode aligned' dataset.\n",
        "    By default, it uses a '--netG unet256' U-Net generator,\n",
        "    a '--netD basic' discriminator (PatchGAN),\n",
        "    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n",
        "\n",
        "    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train=True):\n",
        "        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n",
        "\n",
        "        Parameters:\n",
        "            parser          -- original option parser\n",
        "            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n",
        "\n",
        "        Returns:\n",
        "            the modified parser.\n",
        "\n",
        "        For pix2pix, we do not use image buffer\n",
        "        The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1\n",
        "        By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.\n",
        "        \"\"\"\n",
        "        # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)\n",
        "        parser.set_defaults(norm=\"batch\", netG=\"unet_256\", dataset_mode=\"aligned\")\n",
        "        if is_train:\n",
        "            parser.set_defaults(pool_size=0, gan_mode=\"vanilla\")\n",
        "            parser.add_argument(\"--lambda_L1\", type=float, default=100.0, help=\"weight for L1 loss\")\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"Initialize the pix2pix class.\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        BaseModel.__init__(self, opt)\n",
        "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
        "        self.loss_names = [\"G_GAN\", \"G_L1\", \"D_real\", \"D_fake\"]\n",
        "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
        "        self.visual_names = [\"real_A\", \"fake_B\", \"real_B\"]\n",
        "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
        "        if self.isTrain:\n",
        "            self.model_names = [\"G\", \"D\"]\n",
        "        else:  # during test time, only load G\n",
        "            self.model_names = [\"G\"]\n",
        "        self.device = opt.device\n",
        "        # define networks (both generator and discriminator)\n",
        "        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain)\n",
        "\n",
        "        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n",
        "            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain)\n",
        "\n",
        "        if self.isTrain:\n",
        "            # define loss functions\n",
        "            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # move to the device for custom loss\n",
        "            self.criterionL1 = torch.nn.L1Loss()\n",
        "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
        "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            self.optimizers.append(self.optimizer_G)\n",
        "            self.optimizers.append(self.optimizer_D)\n",
        "\n",
        "    def set_input(self, input):\n",
        "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
        "\n",
        "        Parameters:\n",
        "            input (dict): include the data itself and its metadata information.\n",
        "\n",
        "        The option 'direction' can be used to swap images in domain A and domain B.\n",
        "        \"\"\"\n",
        "        AtoB = self.opt.direction == \"AtoB\"\n",
        "        self.real_A = input[\"A\" if AtoB else \"B\"].to(self.device)\n",
        "        self.real_B = input[\"B\" if AtoB else \"A\"].to(self.device)\n",
        "        self.image_paths = input[\"A_paths\" if AtoB else \"B_paths\"]\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
        "        self.fake_B = self.netG(self.real_A)  # G(A)\n",
        "\n",
        "    def backward_D(self):\n",
        "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
        "        # Fake; stop backprop to the generator by detaching fake_B\n",
        "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
        "        pred_fake = self.netD(fake_AB.detach())\n",
        "        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
        "        # Real\n",
        "        real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
        "        pred_real = self.netD(real_AB)\n",
        "        self.loss_D_real = self.criterionGAN(pred_real, True)\n",
        "        # combine loss and calculate gradients\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "\n",
        "    def backward_G(self):\n",
        "        \"\"\"Calculate GAN and L1 loss for the generator\"\"\"\n",
        "        # First, G(A) should fake the discriminator\n",
        "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
        "        pred_fake = self.netD(fake_AB)\n",
        "        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
        "        # Second, G(A) = B\n",
        "        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
        "        # combine loss and calculate gradients\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "        self.loss_G.backward()\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        self.forward()  # compute fake images: G(A)\n",
        "        # update D\n",
        "        self.set_requires_grad(self.netD, True)  # enable backprop for D\n",
        "        self.optimizer_D.zero_grad()  # set D's gradients to zero\n",
        "        self.backward_D()  # calculate gradients for D\n",
        "        self.optimizer_D.step()  # update D's weights\n",
        "        # update G\n",
        "        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n",
        "        self.optimizer_G.zero_grad()  # set G's gradients to zero\n",
        "        self.backward_G()  # calculate graidents for G\n",
        "        self.optimizer_G.step()  # update G's weights\n"
      ],
      "metadata": {
        "id": "SI2VgFQVIpXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##c) data/aligned_dataset.py → Implement/simplify the dataset class to load the dataset\n"
      ],
      "metadata": {
        "id": "4yzpddeKJnH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from data.base_dataset import BaseDataset, get_params, get_transform\n",
        "from data.image_folder import make_dataset\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class AlignedDataset(BaseDataset):\n",
        "    \"\"\"A dataset class for paired image dataset.\n",
        "\n",
        "    It assumes that the directory '/path/to/data/train' contains image pairs in the form of {A,B}.\n",
        "    During test time, you need to prepare a directory '/path/to/data/test'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"Initialize this dataset class.\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        BaseDataset.__init__(self, opt)\n",
        "        self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory\n",
        "        self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths\n",
        "        assert self.opt.load_size >= self.opt.crop_size  # crop_size should be smaller than the size of loaded image\n",
        "        self.input_nc = self.opt.output_nc if self.opt.direction == \"BtoA\" else self.opt.input_nc\n",
        "        self.output_nc = self.opt.input_nc if self.opt.direction == \"BtoA\" else self.opt.output_nc\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return a data point and its metadata information.\n",
        "\n",
        "        Parameters:\n",
        "            index - - a random integer for data indexing\n",
        "\n",
        "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
        "            A (tensor) - - an image in the input domain\n",
        "            B (tensor) - - its corresponding image in the target domain\n",
        "            A_paths (str) - - image paths\n",
        "            B_paths (str) - - image paths (same as A_paths)\n",
        "        \"\"\"\n",
        "        # read a image given a random integer index\n",
        "        AB_path = self.AB_paths[index]\n",
        "        AB = Image.open(AB_path).convert(\"RGB\")\n",
        "        # split AB image into A and B\n",
        "        w, h = AB.size\n",
        "        w2 = int(w / 2)\n",
        "        A = AB.crop((0, 0, w2, h))\n",
        "        B = AB.crop((w2, 0, w, h))\n",
        "\n",
        "        # apply the same transform to both A and B\n",
        "        transform_params = get_params(self.opt, A.size)\n",
        "        A_transform = get_transform(self.opt, transform_params, grayscale=(self.input_nc == 1))\n",
        "        B_transform = get_transform(self.opt, transform_params, grayscale=(self.output_nc == 1))\n",
        "\n",
        "        A = A_transform(A)\n",
        "        B = B_transform(B)\n",
        "\n",
        "        return {\"A\": A, \"B\": B, \"A_paths\": AB_path, \"B_paths\": AB_path}\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
        "        return len(self.AB_paths)\n"
      ],
      "metadata": {
        "id": "pQV4YejWJsRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##d) options/train_options.py & test_options.py → Replace argparse\n"
      ],
      "metadata": {
        "id": "h1CM_HYIJ5Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we cannot use command-line arguments in Colab, we replace `argparse` with direct attribute assignment on a `TrainOptions()` object.<br><br>\n",
        "\n",
        "### Key Changes:\n",
        "- All hyperparameters (learning rate, epochs, L1 weight, etc.) are now **hard-coded** directly in the training cell.\n",
        "- We manually set every required field (`max_dataset_size`, `continue_train`, `load_iter`, etc.) to avoid `AttributeError`.\n",
        "- This makes the training fully self-contained and reproducible without external scripts or CLI flags — fully compliant with homework rules.<br>"
      ],
      "metadata": {
        "id": "QfdMLzJ1t2wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from options.base_options import BaseOptions\n",
        "\n",
        "\n",
        "class TrainOptions(BaseOptions):\n",
        "    \"\"\"This class includes training options.\n",
        "\n",
        "    It also includes shared options defined in BaseOptions.\n",
        "    \"\"\"\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)\n",
        "        # HTML visualization parameters\n",
        "        parser.add_argument('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n",
        "        parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n",
        "        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
        "        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n",
        "        # network saving and loading parameters\n",
        "        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n",
        "        parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n",
        "        parser.add_argument('--save_by_iter', action='store_true', help='whether saves model by iteration')\n",
        "        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
        "        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
        "        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
        "        # training parameters\n",
        "        parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')\n",
        "        parser.add_argument('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')\n",
        "        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n",
        "        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
        "        parser.add_argument('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n",
        "        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n",
        "        parser.add_argument('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n",
        "        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n",
        "\n",
        "        self.isTrain = True\n",
        "        return parser\n"
      ],
      "metadata": {
        "id": "PFvpNvvgJ5XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from options.base_options import BaseOptions\n",
        "\n",
        "\n",
        "class TestOptions(BaseOptions):\n",
        "    \"\"\"This class includes test options.\n",
        "\n",
        "    It also includes shared options defined in BaseOptions.\n",
        "    \"\"\"\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)  # define shared options\n",
        "        parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n",
        "        parser.add_argument('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n",
        "        parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')\n",
        "        # Dropout and Batchnorm has different behavioir during training and test.\n",
        "        parser.add_argument('--eval', action='store_true', help='use eval mode during test time.')\n",
        "        parser.add_argument('--num_test', type=int, default=50, help='how many test images to run')\n",
        "        # rewrite devalue values\n",
        "        parser.set_defaults(model='test')\n",
        "        # To avoid cropping, the load_size should be the same as crop_size\n",
        "        parser.set_defaults(load_size=parser.get_default('crop_size'))\n",
        "        self.isTrain = False\n",
        "        return parser\n"
      ],
      "metadata": {
        "id": "0HbcWAiEMEVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##e) The entire training loop from train.py → Photo (BtoA) with Rich Debugging"
      ],
      "metadata": {
        "id": "qqWX4ZmYMdxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the complete, standalone training loop for standard pix2pix (semantic labels → realistic photo).\n",
        "\n",
        "### Key Features & Design Decisions:\n",
        "- No external train.py: Everything runs in this cell — fully disassembled and revised.\n",
        "- Rich Debugging (as required in Task 3):\n",
        "  1. Loss printing every 50 iterations\n",
        "  2. Visual comparison (Label)"
      ],
      "metadata": {
        "id": "q8VJ11hIytRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dominate\n",
        "!bash ./datasets/download_pix2pix_dataset.sh facades"
      ],
      "metadata": {
        "id": "dwBu30NkXXTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from options.train_options import TrainOptions\n",
        "from data import create_dataset\n",
        "from models import create_model\n",
        "from util.visualizer import Visualizer\n",
        "\n",
        "# -------------------------------\n",
        "# Setup folders\n",
        "# -------------------------------\n",
        "os.makedirs('./checkpoints', exist_ok=True)\n",
        "os.makedirs('./debug_samples', exist_ok=True)\n",
        "\n",
        "opt = TrainOptions()\n",
        "\n",
        "# <<< SETTINGS >>>\n",
        "opt.dataroot     = './datasets/facades'\n",
        "opt.name         = 'facades_pix2pix_debug'\n",
        "opt.direction    = 'BtoA'          # Label → Photo\n",
        "# <<< ---------------- >>>\n",
        "\n",
        "# === ALL REQUIRED OPTIONS ===\n",
        "opt.model           = 'pix2pix'\n",
        "opt.dataset_mode    = 'aligned'\n",
        "opt.phase           = 'train'\n",
        "opt.isTrain         = True\n",
        "opt.serial_batches  = False\n",
        "opt.num_threads     = 2\n",
        "opt.max_dataset_size = float(\"inf\")\n",
        "\n",
        "opt.netG            = 'unet_256'\n",
        "opt.netD            = 'basic'\n",
        "opt.n_layers_D      = 3\n",
        "opt.input_nc        = 3\n",
        "opt.output_nc       = 3\n",
        "opt.ngf = opt.ndf   = 64\n",
        "opt.norm            = 'batch'\n",
        "opt.no_dropout      = True\n",
        "opt.init_type       = 'normal'\n",
        "opt.init_gain       = 0.02\n",
        "\n",
        "opt.gan_mode        = 'lsgan'\n",
        "opt.lambda_L1       = 100.0\n",
        "opt.batch_size      = 4\n",
        "opt.lr              = 0.0002\n",
        "opt.beta1           = 0.5\n",
        "opt.lr_policy       = 'linear'\n",
        "opt.n_epochs        = 100   #100\n",
        "opt.n_epochs_decay  = 100   #100\n",
        "opt.pool_size       = 50\n",
        "\n",
        "opt.preprocess      = 'resize_and_crop'\n",
        "opt.load_size       = 286\n",
        "opt.crop_size       = 256\n",
        "opt.no_flip         = False\n",
        "\n",
        "opt.checkpoints_dir = './checkpoints'\n",
        "opt.display_freq    = 400\n",
        "opt.print_freq      = 100\n",
        "opt.save_latest_freq = 5000\n",
        "opt.save_epoch_freq = 10\n",
        "opt.no_html         = True\n",
        "opt.continue_train  = False\n",
        "opt.load_iter       = 0\n",
        "opt.epoch           = 'latest'\n",
        "opt.epoch_count     = 1\n",
        "opt.verbose         = False\n",
        "opt.suffix          = ''\n",
        "opt.display_winsize = 256\n",
        "opt.display_id      = 1\n",
        "opt.display_port    = 8097\n",
        "opt.use_wandb       = False\n",
        "\n",
        "opt.gpu_ids = [0] if torch.cuda.is_available() else []\n",
        "opt.device = torch.device(f'cuda:{opt.gpu_ids[0]}' if opt.gpu_ids else 'cpu')\n",
        "\n",
        "# Create experiment folder\n",
        "(Path(opt.checkpoints_dir) / opt.name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Create dataset / model / visualizer\n",
        "# -------------------------------\n",
        "print(\"Creating dataset...\")\n",
        "dataset = create_dataset(opt)\n",
        "print(f\"Training images = {len(dataset)}\")\n",
        "\n",
        "print(\"Creating model...\")\n",
        "model = create_model(opt)\n",
        "model.setup(opt)\n",
        "\n",
        "print(\"Creating visualizer...\")\n",
        "visualizer = Visualizer(opt)\n",
        "\n",
        "# -------------------------------\n",
        "# TRAINING LOOP — FULLY DEBUGGED & BEAUTIFUL\n",
        "# -------------------------------\n",
        "total_iters = 0\n",
        "print(\"\\nTRAINING STARTED — YOU ARE UNSTOPPABLE!\\n\")\n",
        "\n",
        "for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for i, data in enumerate(dataset):\n",
        "        total_iters += opt.batch_size\n",
        "\n",
        "        model.set_input(data)\n",
        "        model.optimize_parameters()\n",
        "\n",
        "        # 1. Print losses every 50 iterations\n",
        "        if total_iters % 50 == 0:\n",
        "            losses = model.get_current_losses()\n",
        "            print(f\"[Epoch {epoch:3d} | Iter {total_iters:6d}] \"\n",
        "                  f\"G_GAN: {losses['G_GAN']:.4f}  G_L1: {losses['G_L1']:.4f}  \"\n",
        "                  f\"D_real: {losses['D_real']:.4f}  D_fake: {losses['D_fake']:.4f}\")\n",
        "\n",
        "        # 2. Show + save generated images every 200 iterations\n",
        "        if total_iters % 200 == 0:\n",
        "            visuals = model.get_current_visuals()\n",
        "            imgs = torch.cat([visuals['real_A'][:4],\n",
        "                              visuals['real_B'][:4],\n",
        "                              visuals['fake_B'][:4]], dim=0)\n",
        "            grid = make_grid(imgs, nrow=4, normalize=True, value_range=(-1, 1))\n",
        "\n",
        "            plt.figure(figsize=(12, 9))\n",
        "            plt.imshow(grid.cpu().permute(1, 2, 0))\n",
        "            plt.title(f\"Label → Real Photo → Generated Photo | Epoch {epoch} | Iter {total_iters}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            save_image(visuals['fake_B'][:4],\n",
        "                       f'./debug_samples/epoch{epoch:03d}_iter{total_iters:06d}.jpg',\n",
        "                       nrow=2, normalize=True, value_range=(-1, 1))\n",
        "\n",
        "        # 3. Print learning rate\n",
        "        if total_iters % 500 == 0:\n",
        "            lr = model.optimizers[0].param_groups[0]['lr']\n",
        "            print(f\"Current Learning Rate: {lr:.7f}\")\n",
        "\n",
        "        # 4. Save model\n",
        "        if total_iters % 1000 == 0:\n",
        "            print(f\"SAVING model at iteration {total_iters}\")\n",
        "            model.save_networks('latest')\n",
        "\n",
        "    # End of epoch\n",
        "    model.update_learning_rate()\n",
        "    print(f\"Epoch {epoch} completed — {time.time() - epoch_start:.1f}s\\n\")\n",
        "\n",
        "print(\"TRAINING FINISHED! Check ./debug_samples/ for your generated facades!\")"
      ],
      "metadata": {
        "id": "8QLcP8SqS5ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final) Reverse Pix2Pix — Photo → Semantic Label Map (AtoB)"
      ],
      "metadata": {
        "id": "GkdziOJ0mIOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the challenging **reverse direction**: training the model to generate semantic label maps from real photos.<br><br>\n",
        "\n",
        "### Why This Is Much Harder:\n",
        "- Original direction (Label→Photo): One-to-many (ambiguous, easier for GANs)\n",
        "- Reverse direction (Photo→Label): Many-to-one (must infer exact segmentation from appearance) — requires strong conditioning and high λ_L1.<br><br>\n",
        "\n",
        "### Key Changes from Forward Model:\n",
        "- `opt.direction = 'AtoB'` → real_A = photo, real_B = label\n",
        "- Input to generator is now real photo → output is label map\n",
        ">- Same U-Net architecture, but now learns inverse mapping\n",
        "- Increased visual frequency (every 400 steps) and saves clean label maps\n",
        "\n",
        "<br><br>\n",
        "\n",
        "This direction often produces surprisingly accurate building layouts after ~100 epochs — demonstrating the power of conditional GANs and skip connections."
      ],
      "metadata": {
        "id": "TjpvbQS_uKxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from options.train_options import TrainOptions\n",
        "from data import create_dataset\n",
        "from models import create_model\n",
        "from util.visualizer import Visualizer\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Setup folders\n",
        "# -------------------------------\n",
        "os.makedirs('./checkpoints', exist_ok=True)\n",
        "os.makedirs('./reverse_samples', exist_ok=True)\n",
        "\n",
        "\n",
        "opt = TrainOptions()\n",
        "\n",
        "# <<< SETTINGS >>>\n",
        "opt.dataroot   = './datasets/facades'\n",
        "opt.name       = 'facades_photo2label'\n",
        "opt.direction  = 'AtoB'\n",
        "# <<< ------------------ >>>\n",
        "\n",
        "# === ALL REQUIRED OPTIONS ===\n",
        "opt.model               = 'pix2pix'\n",
        "opt.dataset_mode        = 'aligned'\n",
        "opt.phase               = 'train'\n",
        "opt.isTrain             = True\n",
        "opt.serial_batches      = False\n",
        "opt.num_threads         = 2\n",
        "opt.max_dataset_size    = float(\"inf\")\n",
        "\n",
        "opt.netG                = 'unet_256'\n",
        "opt.netD                = 'basic'\n",
        "opt.n_layers_D          = 3\n",
        "opt.input_nc            = 3\n",
        "opt.output_nc           = 3\n",
        "opt.ngf = opt.ndf       = 64\n",
        "opt.norm                = 'batch'\n",
        "opt.no_dropout          = True\n",
        "opt.init_type           = 'normal'\n",
        "opt.init_gain           = 0.02\n",
        "\n",
        "opt.gan_mode            = 'lsgan'\n",
        "opt.lambda_L1           = 100.0\n",
        "opt.batch_size          = 4\n",
        "opt.lr                  = 0.0002\n",
        "opt.beta1               = 0.5\n",
        "opt.lr_policy           = 'linear'\n",
        "opt.n_epochs            = 100   #100\n",
        "opt.n_epochs_decay      = 100   #100\n",
        "opt.pool_size           = 50\n",
        "\n",
        "opt.preprocess          = 'resize_and_crop'\n",
        "opt.load_size           = 286\n",
        "opt.crop_size           = 256\n",
        "opt.no_flip             = False\n",
        "\n",
        "opt.checkpoints_dir     = './checkpoints'\n",
        "opt.display_freq        = 400\n",
        "opt.print_freq          = 100\n",
        "opt.save_latest_freq    = 5000\n",
        "opt.save_epoch_freq     = 10\n",
        "opt.no_html             = True\n",
        "opt.continue_train      = False\n",
        "opt.load_iter           = 0\n",
        "opt.epoch               = 'latest'\n",
        "opt.epoch_count         = 1\n",
        "opt.verbose             = False\n",
        "opt.suffix              = ''\n",
        "opt.display_winsize     = 256\n",
        "opt.display_id          = 1\n",
        "opt.display_port        = 8097\n",
        "opt.use_wandb           = False\n",
        "\n",
        "opt.gpu_ids = [0] if torch.cuda.is_available() else []\n",
        "opt.device = torch.device(f'cuda:{opt.gpu_ids[0]}' if opt.gpu_ids else 'cpu')\n",
        "\n",
        "# Create experiment folder\n",
        "(Path(opt.checkpoints_dir) / opt.name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Create everything\n",
        "# -------------------------------\n",
        "print(\"Creating dataset (Photo → Facade Label)...\")\n",
        "dataset = create_dataset(opt)\n",
        "print(f\"Training images = {len(dataset)}\")\n",
        "\n",
        "print(\"Creating model...\")\n",
        "model = create_model(opt)\n",
        "model.setup(opt)\n",
        "\n",
        "print(\"Creating visualizer...\")\n",
        "visualizer = Visualizer(opt)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. TRAINING LOOP — REVERSE DIRECTION WITH NICE VISUALS\n",
        "# -------------------------------\n",
        "total_iters = 0\n",
        "print(\"\\nREVERSE PIX2PIX TRAINING STARTED: Photo → Facade Label Map\\n\")\n",
        "\n",
        "for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for i, data in enumerate(dataset):\n",
        "        total_iters += opt.batch_size\n",
        "\n",
        "        model.set_input(data)\n",
        "        model.optimize_parameters()\n",
        "\n",
        "        # Print losses every 100 steps\n",
        "        if total_iters % 100 == 0:\n",
        "            losses = model.get_current_losses()\n",
        "            print(f\"[Epoch {epoch:3d} | Iter {total_iters:6d}] \"\n",
        "                  f\"G_GAN: {losses['G_GAN']:.4f} | G_L1: {losses['G_L1']:.4f} | \"\n",
        "                  f\"D_real: {losses['D_real']:.4f} | D_fake: {losses['D_fake']:.4f}\")\n",
        "\n",
        "        # Show & save results every 400 steps\n",
        "        if total_iters % 400 == 0:\n",
        "            visuals = model.get_current_visuals()\n",
        "\n",
        "            # Concatenate: Photo → Generated Label → Ground Truth Label\n",
        "            imgs = torch.cat([visuals['real_A'][:3],\n",
        "                              visuals['fake_B'][:3],\n",
        "                              visuals['real_B'][:3]], dim=0)\n",
        "            grid = make_grid(imgs, nrow=3, normalize=True, value_range=(-1,1))\n",
        "\n",
        "            plt.figure(figsize=(15, 10))\n",
        "            plt.imshow(grid.cpu().permute(1, 2, 0))\n",
        "            plt.title(f\"Photo → Generated Label → GT | Epoch {epoch} | Iter {total_iters}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            # Save generated label maps\n",
        "            save_image(visuals['fake_B'][:3],\n",
        "                       f'./reverse_samples/epoch{epoch:03d}_iter{total_iters:06d}.png',\n",
        "                       nrow=3, normalize=True, value_range=(-1,1))\n",
        "\n",
        "        # Save model\n",
        "        if total_iters % 5000 == 0:\n",
        "            print(f\"Saving model at iteration {total_iters}\")\n",
        "            model.save_networks('latest')\n",
        "\n",
        "    model.update_learning_rate()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Saving checkpoint at epoch {epoch}\")\n",
        "        model.save_networks('latest')\n",
        "        model.save_networks(str(epoch))\n",
        "\n",
        "    print(f\"Epoch {epoch} finished — {time.time()-epoch_start:.1f}s\\n\")\n",
        "\n",
        "print(\"\\nREVERSE TRAINING COMPLETE! Check ./reverse_samples/ for generated label maps!\")"
      ],
      "metadata": {
        "id": "ZgS60aA9mKEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}